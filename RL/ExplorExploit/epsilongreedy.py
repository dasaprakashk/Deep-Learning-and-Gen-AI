import numpy as np
import matplotlib.pyplot as plt

N_TRIALS = 10000
EPS = 0.1
BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]

class Bandit:
    def __init__(self, p):
        self.p = p
        self.p_estimate = 0
        self.N = 0
    
    def pull(self):
        return np.random.random() < self.p

    def update(self, x):
        self.N += 1
        self.p_estimate = ((self.N-1)*self.p_estimate + x)/self.N

def experiment():
    bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]
    rewards = np.zeros(N_TRIALS)
    n_explored = 0
    n_exploited = 0
    n_optimal = 0
    optimal_j = np.argmax([b.p for b in bandits])
    print("optimal_j", optimal_j)

    for i in range(N_TRIALS):
        # User epsilon - greedy to select the next bandit
        if np.random.random() < EPS:
            n_explored += 1
            j = np.random.randint(len(bandits))
        else:
            n_exploited += 1
            j = np.argmax([b.p_estimate for b in bandits])
        
        if j == optimal_j:
            n_optimal += 1
        
        #pull the arm of the bandit with largest sample
        x = bandits[j].pull()

        #update rewards
        rewards[i] = x

        #update the distribution of the bandit we just pulled
        bandits[j].update(x)
    
    # print mean estimate for bandits 
    for bandit in bandits:
        print("mean_estimate:", bandit.p_estimate)
    
    #print metrics
    print("total reward earned", rewards.sum())
    print("overall win rate", rewards.sum()/N_TRIALS)
    print("number of explorations", n_explored)
    print("number of exploitations", n_exploited)
    print("number of times optimal bandit selected", n_optimal)

    #plot
    cumulative_rewards = np.cumsum(rewards)
    win_rates = cumulative_rewards/(np.arange(N_TRIALS)+1)
    plt.ylim([0,1])
    plt.plot(win_rates)
    plt.plot(np.ones(N_TRIALS)*np.max(BANDIT_PROBABILITIES))
    plt.show()

if __name__ == '__main__':
    experiment()